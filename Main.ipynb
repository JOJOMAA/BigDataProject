{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cf04f25b6738a3",
   "metadata": {},
   "source": [
    "## Demografische Entwicklung Wiens seit 2008: Analyse der\n",
    "## Bevölkerungsstruktur und Geburtenentwicklung auf\n",
    "## Bezirksebene\n",
    "<u>**Big Data Projekt von:**</u>\n",
    "<br>\n",
    "Johannes Reitterer <br>\n",
    "Johannes Mantler <br>\n",
    "Nicolas Nemeth <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be410d77233692",
   "metadata": {},
   "source": [
    "# ETL Pipeline\n",
    "Diese ETL-Pipeline lädt demografische Daten der Stadt Wien, bereinigt sie und speichert sie in MongoDB zur weiteren Analyse.\n",
    "\n",
    "**Datenquellen:**\n",
    "- Bevölkerung nach Geburtsbundesland (2008-heute): ~500.000 Datensätze https://www.data.gv.at/datasets/f54e6828-3d75-4a82-89cb-23c58057bad4?locale=de\n",
    "- Geburtenstatistik (2002-heute): ~50.000 Datensätze https://www.data.gv.at/datasets/f54e6828-3d75-4a82-89cb-23c58057bad4?locale=de\n",
    "\n",
    "## Pipeline-Ablauf\n",
    "\n",
    "### 1. Extract (Daten laden)\n",
    "\n",
    "Die Rohdaten werden aus CSV-Dateien von data.gv.at geladen.\n",
    "\n",
    "Da es Probleme bei der API-Abfrage gibt, müssen die csv files manuell gedownloaded werden und in den Projekt Ordner eingefügt werden.\n",
    "\n",
    "### 2. Transform (Daten bereinigen)\n",
    "\n",
    "**Spaltenumbenennung:**\n",
    "- Englische Spaltennamen werden zu deutschen Namen konvertiert\n",
    "- Beispiel: `REF_YEAR` → `Jahr`, `DISTRICT_CODE` → `Bezirk_Roh`\n",
    "\n",
    "**Bezirkscode-Transformation:**\n",
    "\n",
    "Wien verwendet statistische Codes (90101-90223), die zu Postleitzahlen konvertiert werden:\n",
    "\n",
    "```\n",
    "90101 → 1010 (1. Bezirk)\n",
    "90201 → 1020 (2. Bezirk)\n",
    "90301 → 1030 (3. Bezirk)\n",
    "...\n",
    "```\n",
    "\n",
    "**Datenbereinigung:**\n",
    "- Ungültige Bezirkscodes entfernen\n",
    "- Fehlende Werte mit 0 auffüllen\n",
    "- Negative Werte korrigieren\n",
    "- Datentypen zu Integer konvertieren\n",
    "\n",
    "(1 = Männer, 2 = Frauen)\n",
    "\n",
    "### 3. Load (Daten speichern)\n",
    "\n",
    "Die bereinigten Daten werden in MongoDB gespeichert:\n",
    "- **Collection `population`**: Bevölkerungsdaten nach Bezirk, Jahr, Alter, Geschlecht und Herkunft\n",
    "- **Collection `births`**: Geburtendaten nach Bezirk, Jahr und Geschlecht\n",
    " \n",
    "**Dokumentstruktur Beispiel:**\n",
    "```json\n",
    "{\n",
    "  \"Jahr\": 2020,\n",
    "  \"Bezirk\": 1010,\n",
    "  \"Geschlecht\": 1,\n",
    "  \"Alter\": 25,\n",
    "  \"Wien\": 1234,\n",
    "  \"Ausland\": 789\n",
    "}\n",
    "```\n",
    "\n",
    "## Verwendung\n",
    "\n",
    "```python\n",
    "# Pipeline ausführen\n",
    "run_pipeline()\n",
    "\n",
    "# Ergebnis: Daten in MongoDB unter wien_demografie_db\n",
    "# - population: Bevölkerungsdaten\n",
    "# - births: Geburtendaten\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "525e698f13ddee9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T21:36:21.729134Z",
     "start_time": "2026-01-08T21:36:20.604481Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Authors: Johannes Mantler, Johannes Reitterer, Nicolas Nemeth\n",
    "\n",
    "Data Sources: \n",
    "- Population by province of birth (2008-present) https://www.data.gv.at/datasets/98b782ca-8e46-43d7-a061-e196d0e0160a?locale=de\n",
    "- Birth statistics (2002-present) https://www.data.gv.at/datasets/f54e6828-3d75-4a82-89cb-23c58057bad4?locale=de\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "MONGO_CONFIG = {\n",
    "    'uri': \"mongodb://admin:admin123@localhost:27017/\",\n",
    "    'auth_source': \"admin\",\n",
    "    'database': \"wien_demografie_db\",\n",
    "    'use_docker': True\n",
    "}\n",
    "\n",
    "#Da Probleme beim automatisiertem Download, direkt CSV Files im Projekt \n",
    "#URL_BEVOELKERUNG = \"https://www.wien.gv.at/gogv/l9ogdviebezpopsexage5stkcobgeoat102008f\"\n",
    "#URL_GEBURTEN = \"https://www.wien.gv.at/gogv/l9ogdviebezpopsexbir2002f\"\n",
    "\n",
    "DATA_FILES = {\n",
    "    'population': 'vie-bez-pop-sex-age5-stk-cob-geoat10-2008f.csv',\n",
    "    'births': 'vie-bez-pop-sex-bir-2002f.csv'\n",
    "}\n",
    "\n",
    "POPULATION_COLUMNS = {\n",
    "    'REF_YEAR': 'Jahr',\n",
    "    'DISTRICT_CODE': 'Bezirk_Roh',\n",
    "    'SUB_DISTRICT_CODE': 'Sub_Bezirk',\n",
    "    'REF_DATE': 'Datum',\n",
    "    'SEX': 'Geschlecht',\n",
    "    'AGE1': 'Alter',\n",
    "    'UNK': 'Unbekannt',\n",
    "    'BGD': 'Burgenland',\n",
    "    'KTN': 'Kaernten',\n",
    "    'NOE': 'Niederoesterreich',\n",
    "    'OOE': 'Oberoesterreich',\n",
    "    'SBG': 'Salzburg',\n",
    "    'STK': 'Steiermark',\n",
    "    'TIR': 'Tirol',\n",
    "    'VBG': 'Vorarlberg',\n",
    "    'VIE': 'Wien',\n",
    "    'FOR': 'Ausland'\n",
    "}\n",
    "\n",
    "BIRTH_COLUMNS = {\n",
    "    'REF_YEAR': 'Jahr',\n",
    "    'DISTRICT_CODE': 'Bezirk_Roh',\n",
    "    'SUB_DISTRICT_CODE': 'Sub_Bezirk',\n",
    "    'REF_DATE': 'Datum',\n",
    "    'SEX': 'Geschlecht',\n",
    "    'BIR': 'Anzahl_Geburten'\n",
    "}\n",
    "\n",
    "BUNDESLAND_COLUMNS = [\n",
    "    'Unbekannt', 'Burgenland', 'Kaernten', 'Niederoesterreich',\n",
    "    'Oberoesterreich', 'Salzburg', 'Steiermark', 'Tirol',\n",
    "    'Vorarlberg', 'Wien', 'Ausland'\n",
    "]\n",
    "\n",
    "\n",
    "def setup_mongodb():\n",
    "    try:\n",
    "        if MONGO_CONFIG['use_docker']:\n",
    "            client = MongoClient(\n",
    "                MONGO_CONFIG['uri'],\n",
    "                serverSelectionTimeoutMS=5000,\n",
    "                authSource=MONGO_CONFIG['auth_source']\n",
    "            )\n",
    "        else:\n",
    "            client = MongoClient(\n",
    "                MONGO_CONFIG['uri'],\n",
    "                serverSelectionTimeoutMS=5000\n",
    "            )\n",
    "\n",
    "        client.server_info()\n",
    "        db = client[MONGO_CONFIG['database']]\n",
    "        return client, db\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: MongoDB connection failed - {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def clean_district_code(code):\n",
    "    try:\n",
    "        code_str = str(code).strip()\n",
    "\n",
    "        if code_str.startswith('9') and len(code_str) == 5:\n",
    "            district_num = int(code_str[1:3])\n",
    "            return 1000 + district_num * 10\n",
    "\n",
    "        if code_str.startswith('1') and len(code_str) == 4:\n",
    "            return int(code_str)\n",
    "\n",
    "        return int(code_str)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def extract_data():\n",
    "    df_pop = pd.read_csv(\n",
    "        DATA_FILES['population'],\n",
    "        sep=';',\n",
    "        encoding='utf-8-sig',\n",
    "        skiprows=1,\n",
    "        header=0\n",
    "    )\n",
    "\n",
    "    df_birth = pd.read_csv(\n",
    "        DATA_FILES['births'],\n",
    "        sep=';',\n",
    "        encoding='utf-8-sig',\n",
    "        skiprows=1,\n",
    "        header=0\n",
    "    )\n",
    "\n",
    "    return df_pop, df_birth\n",
    "\n",
    "\n",
    "\n",
    "def transform_population_data(df):\n",
    "    rename_map = {k: v for k, v in POPULATION_COLUMNS.items() if k in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    if 'Sub_Bezirk' in df.columns:\n",
    "        df['Bezirk'] = df['Sub_Bezirk'].apply(clean_district_code)\n",
    "    elif 'Bezirk_Roh' in df.columns:\n",
    "        df['Bezirk'] = df['Bezirk_Roh'].apply(clean_district_code)\n",
    "    else:\n",
    "        print(\"  WARNING: No district code column found\")\n",
    "        df['Bezirk'] = 0\n",
    "\n",
    "    df = df[df['Bezirk'] > 0]\n",
    "\n",
    "    for col in BUNDESLAND_COLUMNS:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    if 'Jahr' in df.columns:\n",
    "        df['Jahr'] = pd.to_numeric(df['Jahr'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_birth_data(df):\n",
    "    rename_map = {k: v for k, v in BIRTH_COLUMNS.items() if k in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    if 'Sub_Bezirk' in df.columns:\n",
    "        df['Bezirk'] = df['Sub_Bezirk'].apply(clean_district_code)\n",
    "    elif 'Bezirk_Roh' in df.columns:\n",
    "        df['Bezirk'] = df['Bezirk_Roh'].apply(clean_district_code)\n",
    "    else:\n",
    "        df['Bezirk'] = 0\n",
    "\n",
    "\n",
    "\n",
    "    df = df[df['Bezirk'] > 0]\n",
    "\n",
    "    if 'Anzahl_Geburten' in df.columns:\n",
    "        df['Anzahl_Geburten'] = pd.to_numeric(\n",
    "            df['Anzahl_Geburten'],\n",
    "            errors='coerce'\n",
    "        ).fillna(0).astype(int)\n",
    "\n",
    "    if 'Jahr' in df.columns:\n",
    "        df['Jahr'] = pd.to_numeric(df['Jahr'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_data_sources(df_pop, df_birth):\n",
    "    pop_agg = df_pop.groupby(['Jahr', 'Bezirk']).agg({\n",
    "        'Wien': 'sum',\n",
    "        'Ausland': 'sum',\n",
    "        'Geschlecht': 'count'\n",
    "    }).reset_index()\n",
    "\n",
    "    pop_agg.rename(columns={'Geschlecht': 'Gesamt_Bevoelkerung'}, inplace=True)\n",
    "\n",
    "    birth_agg = df_birth.groupby(['Jahr', 'Bezirk']).agg({\n",
    "        'Anzahl_Geburten': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    merged = pd.merge(pop_agg, birth_agg, on=['Jahr', 'Bezirk'], how='outer')\n",
    "    merged = merged.fillna(0)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def transform_data(df_pop, df_birth):\n",
    "    df_pop_clean = transform_population_data(df_pop)\n",
    "    df_birth_clean = transform_birth_data(df_birth)\n",
    "    df_merged = merge_data_sources(df_pop_clean, df_birth_clean)\n",
    "    return df_pop_clean, df_birth_clean, df_merged\n",
    "\n",
    "\n",
    "def load_data(db, df_pop, df_birth, df_merged):\n",
    "    db.population.delete_many({})\n",
    "    db.births.delete_many({})\n",
    "    db.merged_analysis.delete_many({})\n",
    "\n",
    "    if len(df_pop) > 0:\n",
    "        db.population.insert_many(df_pop.to_dict(\"records\"))\n",
    "\n",
    "    if len(df_birth) > 0:\n",
    "        db.births.insert_many(df_birth.to_dict(\"records\"))\n",
    "\n",
    "    if len(df_merged) > 0:\n",
    "        db.merged_analysis.insert_many(df_merged.to_dict(\"records\"))\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    client, db = setup_mongodb()\n",
    "    try:\n",
    "        df_pop, df_birth = extract_data()\n",
    "        df_pop_clean, df_birth_clean, df_merged = transform_data(df_pop, df_birth)\n",
    "        load_data(db, df_pop_clean, df_birth_clean, df_merged)\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "\n",
    "run_pipeline()\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "955abe1cc6f4e28a",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "bd58c337c5a90c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T21:36:26.359111Z",
     "start_time": "2026-01-08T21:36:25.651761Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Wien-Geburten-Zeitverlauf\")\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.6.0\")\n",
    "    .config(\n",
    "        \"spark.mongodb.read.connection.uri\",\n",
    "        \"mongodb://admin:admin123@localhost:27017/wien_demografie_db?authSource=admin\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)  # muss 3.5.x sein\n"
   ],
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\\hadoop does not exist -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1305)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1291)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:417)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1864)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:545)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:545)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:545)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\\hadoop does not exist -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: Hadoop home directory C:\\hadoop does not exist\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:544)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msql\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[32m      3\u001B[39m spark = (\n\u001B[32m      4\u001B[39m     \u001B[43mSparkSession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuilder\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWien-Geburten-Zeitverlauf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mspark.jars.packages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43morg.mongodb.spark:mongo-spark-connector_2.12:10.6.0\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mspark.mongodb.read.connection.uri\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmongodb://admin:admin123@localhost:27017/wien_demografie_db?authSource=admin\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m )\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(spark.version)  \u001B[38;5;66;03m# muss 3.5.x sein\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Informatik\\SEMESTER_5\\Computer Vision and NAtural Language Processing\\OpenCV Demo-20250912\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001B[39m, in \u001B[36mSparkSession.Builder.getOrCreate\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    554\u001B[39m     sparkConf.set(key, value)\n\u001B[32m    555\u001B[39m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m sc = \u001B[43mSparkContext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[32m    558\u001B[39m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[32m    559\u001B[39m session = SparkSession(sc, options=\u001B[38;5;28mself\u001B[39m._options)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Informatik\\SEMESTER_5\\Computer Vision and NAtural Language Processing\\OpenCV Demo-20250912\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001B[39m, in \u001B[36mSparkContext.getOrCreate\u001B[39m\u001B[34m(cls, conf)\u001B[39m\n\u001B[32m    521\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext._lock:\n\u001B[32m    522\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m523\u001B[39m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    524\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    525\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext._active_spark_context\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Informatik\\SEMESTER_5\\Computer Vision and NAtural Language Processing\\OpenCV Demo-20250912\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:207\u001B[39m, in \u001B[36mSparkContext.__init__\u001B[39m\u001B[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[39m\n\u001B[32m    205\u001B[39m SparkContext._ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway=gateway, conf=conf)\n\u001B[32m    206\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m207\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    208\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    209\u001B[39m \u001B[43m        \u001B[49m\u001B[43mappName\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    210\u001B[39m \u001B[43m        \u001B[49m\u001B[43msparkHome\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpyFiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    212\u001B[39m \u001B[43m        \u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    213\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatchSize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    215\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    216\u001B[39m \u001B[43m        \u001B[49m\u001B[43mjsc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprofiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[43mudf_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmemory_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m.stop()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Informatik\\SEMESTER_5\\Computer Vision and NAtural Language Processing\\OpenCV Demo-20250912\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:300\u001B[39m, in \u001B[36mSparkContext._do_init\u001B[39m\u001B[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[39m\n\u001B[32m    297\u001B[39m \u001B[38;5;28mself\u001B[39m.environment[\u001B[33m\"\u001B[39m\u001B[33mPYTHONHASHSEED\u001B[39m\u001B[33m\"\u001B[39m] = os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mPYTHONHASHSEED\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m0\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    299\u001B[39m \u001B[38;5;66;03m# Create the Java SparkContext through Py4J\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m300\u001B[39m \u001B[38;5;28mself\u001B[39m._jsc = jsc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_initialize_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conf\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_jconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    301\u001B[39m \u001B[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[39;00m\n\u001B[32m    302\u001B[39m \u001B[38;5;28mself\u001B[39m._conf = SparkConf(_jconf=\u001B[38;5;28mself\u001B[39m._jsc.sc().conf())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Informatik\\SEMESTER_5\\Computer Vision and NAtural Language Processing\\OpenCV Demo-20250912\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:429\u001B[39m, in \u001B[36mSparkContext._initialize_context\u001B[39m\u001B[34m(self, jconf)\u001B[39m\n\u001B[32m    425\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    426\u001B[39m \u001B[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001B[39;00m\n\u001B[32m    427\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    428\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m._jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m429\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jvm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mJavaSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjconf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Informatik\\SEMESTER_5\\Computer Vision and NAtural Language Processing\\OpenCV Demo-20250912\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1627\u001B[39m, in \u001B[36mJavaClass.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1621\u001B[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001B[32m   1622\u001B[39m     \u001B[38;5;28mself\u001B[39m._command_header +\\\n\u001B[32m   1623\u001B[39m     args_command +\\\n\u001B[32m   1624\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1626\u001B[39m answer = \u001B[38;5;28mself\u001B[39m._gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1627\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1628\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_gateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1630\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1631\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Informatik\\SEMESTER_5\\Computer Vision and NAtural Language Processing\\OpenCV Demo-20250912\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    325\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    328\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    329\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    332\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    333\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\\hadoop does not exist -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1305)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1291)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:417)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1864)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:545)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:545)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:545)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\\hadoop does not exist -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: Hadoop home directory C:\\hadoop does not exist\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:544)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258f8772de780236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T20:51:59.201842Z",
     "start_time": "2026-01-08T20:51:46.682366Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+-----+----------+----+----+----------+--------------------+\n",
      "|Anzahl_Geburten|Bezirk|Bezirk_Roh|Datum|Geschlecht|Jahr|NUTS|Sub_Bezirk|                 _id|\n",
      "+---------------+------+----------+-----+----------+----+----+----------+--------------------+\n",
      "|             58|  1010|     90100| 2002|         1|2002|AT13|     90100|69601f2050812e9d0...|\n",
      "|             72|  1010|     90100| 2002|         2|2002|AT13|     90100|69601f2050812e9d0...|\n",
      "|            490|  1020|     90200| 2002|         1|2002|AT13|     90200|69601f2050812e9d0...|\n",
      "|            491|  1020|     90200| 2002|         2|2002|AT13|     90200|69601f2050812e9d0...|\n",
      "|            370|  1030|     90300| 2002|         1|2002|AT13|     90300|69601f2050812e9d0...|\n",
      "+---------------+------+----------+-----+----------+----+----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "births_df = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.collection\", \"births\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "births_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de09dbae-bdd2-442d-9822-e8dee9a41ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AGE5: integer (nullable = true)\n",
      " |-- Ausland: integer (nullable = true)\n",
      " |-- Bezirk: integer (nullable = true)\n",
      " |-- Bezirk_Roh: integer (nullable = true)\n",
      " |-- Burgenland: integer (nullable = true)\n",
      " |-- Datum: integer (nullable = true)\n",
      " |-- Geschlecht: integer (nullable = true)\n",
      " |-- Jahr: integer (nullable = true)\n",
      " |-- Kaernten: integer (nullable = true)\n",
      " |-- NUTS: string (nullable = true)\n",
      " |-- Niederoesterreich: integer (nullable = true)\n",
      " |-- Oberoesterreich: integer (nullable = true)\n",
      " |-- Salzburg: integer (nullable = true)\n",
      " |-- Steiermark: integer (nullable = true)\n",
      " |-- Sub_Bezirk: integer (nullable = true)\n",
      " |-- Tirol: integer (nullable = true)\n",
      " |-- Unbekannt: integer (nullable = true)\n",
      " |-- Vorarlberg: integer (nullable = true)\n",
      " |-- Wien: integer (nullable = true)\n",
      " |-- _id: string (nullable = true)\n",
      "\n",
      "+----+-------+------+----------+----------+--------+----------+----+--------+----+-----------------+---------------+--------+----------+----------+-----+---------+----------+----+--------------------+\n",
      "|AGE5|Ausland|Bezirk|Bezirk_Roh|Burgenland|   Datum|Geschlecht|Jahr|Kaernten|NUTS|Niederoesterreich|Oberoesterreich|Salzburg|Steiermark|Sub_Bezirk|Tirol|Unbekannt|Vorarlberg|Wien|                 _id|\n",
      "+----+-------+------+----------+----------+--------+----------+----+--------+----+-----------------+---------------+--------+----------+----------+-----+---------+----------+----+--------------------+\n",
      "|   1|     39|  1010|     90100|         0|20080101|         1|2008|       1|AT13|                9|              1|       0|         4|     90100|    0|        0|         0| 207|69601f2050812e9d0...|\n",
      "|   2|     44|  1010|     90100|         0|20080101|         1|2008|       1|AT13|                3|              2|       1|         1|     90100|    0|        0|         0| 202|69601f2050812e9d0...|\n",
      "|   3|     40|  1010|     90100|         0|20080101|         1|2008|       2|AT13|                7|              4|       1|         0|     90100|    1|        1|         2| 198|69601f2050812e9d0...|\n",
      "|   4|     72|  1010|     90100|         2|20080101|         1|2008|       2|AT13|                7|              3|       2|         3|     90100|    1|        3|         2| 201|69601f2050812e9d0...|\n",
      "|   5|    138|  1010|     90100|         2|20080101|         1|2008|       1|AT13|               18|              6|       3|         6|     90100|    4|        2|         1| 253|69601f2050812e9d0...|\n",
      "+----+-------+------+----------+----------+--------+----------+----+--------+----+-----------------+---------------+--------+----------+----------+-----+---------+----------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "population_df = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.collection\", \"population\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "population_df.printSchema()\n",
    "population_df.show(5)\n",
    "\n",
    "\n",
    "pop_origin_df = (\n",
    "    population_df\n",
    "    .groupBy(\"Jahr\", \"Bezirk\")\n",
    "    .agg(\n",
    "        F.sum(\"Ausland\").alias(\"Bev_Ausland\"),\n",
    "        F.sum(\"Wien\").alias(\"Bev_Wien\")\n",
    "    )\n",
    ")\n",
    "\n",
    "births_agg_df = (\n",
    "    births_df\n",
    "    .groupBy(\"Jahr\", \"Bezirk\")\n",
    "    .agg(F.sum(\"Anzahl_Geburten\").alias(\"Geburten\"))\n",
    ")\n",
    "\n",
    "birth_rate_origin_df = (\n",
    "    births_agg_df\n",
    "    .join(pop_origin_df, [\"Jahr\", \"Bezirk\"])\n",
    "    .withColumn(\n",
    "        \"Geburtenrate_Ausland\",\n",
    "        F.col(\"Geburten\") / F.col(\"Bev_Ausland\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Geburtenrate_Wien\",\n",
    "        F.col(\"Geburten\") / F.col(\"Bev_Wien\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a0d7f3-22da-4b8c-b9d3-53cfe42ad400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+-------------------+\n",
      "|Bezirk|Bev_Ausland|Bev_Gesamt|   Auslaenderanteil|\n",
      "+------+-----------+----------+-------------------+\n",
      "|  1150|     625236|   1337225|0.46756230252949205|\n",
      "|  1200|     657751|   1511808|0.43507575035983403|\n",
      "|  1050|     413544|    964825|0.42862073433005987|\n",
      "|  1100|    1423243|   3488017| 0.4080378622007863|\n",
      "|  1160|     727318|   1785032|  0.407453759932595|\n",
      "|  1020|     732516|   1819078|0.40268531640754274|\n",
      "|  1120|     654201|   1676218| 0.3902839606781457|\n",
      "|  1040|     217183|    573607|0.37862682986783636|\n",
      "|  1170|     370226|    984228|0.37615877621851845|\n",
      "|  1030|     594349|   1597165|0.37212748839349724|\n",
      "+------+-----------+----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "zuwanderung_df = (\n",
    "    population_df\n",
    "    .groupBy(\"Bezirk\")\n",
    "    .agg(\n",
    "        F.sum(\"Ausland\").alias(\"Bev_Ausland\"),\n",
    "        F.sum(\n",
    "            F.col(\"Ausland\") +\n",
    "            F.col(\"Wien\") +\n",
    "            F.col(\"Burgenland\") +\n",
    "            F.col(\"Kaernten\") +\n",
    "            F.col(\"Niederoesterreich\") +\n",
    "            F.col(\"Oberoesterreich\") +\n",
    "            F.col(\"Salzburg\") +\n",
    "            F.col(\"Steiermark\") +\n",
    "            F.col(\"Tirol\") +\n",
    "            F.col(\"Vorarlberg\")\n",
    "        ).alias(\"Bev_Gesamt\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Auslaenderanteil\",\n",
    "        F.col(\"Bev_Ausland\") / F.col(\"Bev_Gesamt\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"Auslaenderanteil\"))\n",
    ")\n",
    "\n",
    "zuwanderung_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc81462c-74f9-4088-986e-67a7b6614414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+-----------+-------------------------+\n",
      "|Jahr|Bezirk|Geburten|Bev_Ausland|Geburten_pro_1000_Ausland|\n",
      "+----+------+--------+-----------+-------------------------+\n",
      "|2008|  1230|     836|      16493|        50.68817073910144|\n",
      "|2008|  1220|    1485|      30923|        48.02250751867542|\n",
      "|2010|  1230|     833|      17476|        47.66536964980544|\n",
      "|2011|  1230|     843|      17994|       46.848949649883295|\n",
      "|2012|  1230|     869|      18663|       46.562717676686496|\n",
      "|2014|  1230|     927|      20036|        46.26671990417249|\n",
      "|2011|  1220|    1545|      34303|        45.03979243797918|\n",
      "|2013|  1130|     473|      10511|        45.00047569213205|\n",
      "|2012|  1220|    1608|      35928|         44.7561790247161|\n",
      "|2015|  1220|    1875|      42016|        44.62585681645088|\n",
      "+----+------+--------+-----------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births_agg_df = (\n",
    "    births_df\n",
    "    .groupBy(\"Jahr\", \"Bezirk\")\n",
    "    .agg(F.sum(\"Anzahl_Geburten\").alias(\"Geburten\"))\n",
    ")\n",
    "\n",
    "migration_birth_df = (\n",
    "    births_agg_df\n",
    "    .join(\n",
    "        population_df.groupBy(\"Jahr\", \"Bezirk\")\n",
    "        .agg(F.sum(\"Ausland\").alias(\"Bev_Ausland\")),\n",
    "        [\"Jahr\", \"Bezirk\"]\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Geburten_pro_1000_Ausland\",\n",
    "        (F.col(\"Geburten\") / F.col(\"Bev_Ausland\")) * 1000\n",
    "    )\n",
    "    .orderBy(F.desc(\"Geburten_pro_1000_Ausland\"))\n",
    ")\n",
    "\n",
    "migration_birth_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438674a7-d20d-42ee-a257-b749d5849b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
